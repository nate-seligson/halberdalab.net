<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>JHU VCL - HalberdaSiresFeigenson2006</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/stylish-portfolio.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,300italic,400italic,700italic" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <!-- Services -->
    <!-- The circle icons use Font Awesome's stacked icon classes. For more information, visit http://fontawesome.io/examples/ -->
    <section class="services bg-primary">
        <div class="container">
            <div class="row text-center">
                <div class="col-lg-10 col-lg-offset-1">
                    <h2>Halberda, Sires, &amp; Feigenson (2006) <br> Multiple spatially-overlapping sets can be enumerated in parallel </h2>
                    <hr class="small">
                    <div class="row">
                        <div class="col-md-12 col-sm-12">
                            <div class="service-item">
                                <span class="fa-stack fa-3x">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <a href="index.html"><i class="fa fa-home fa-stack-1x text-primary link-light"></i></a>
                            </span>
                                <h4>
                                <strong>VCL Home</strong>
                                </h4>
                            </div>
                        </div>
                    </div>
                    <!-- /.row (nested) -->
                </div>
                <!-- /.col-lg-10 -->
            </div>
            <!-- /.row -->
        </div>
        <!-- /.container -->
    </section>
        

        <!-- /.container -->
    </section>


    <!-- Call to Action 
    <aside id="panamath" class="call-to-action bg-primary">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h3>Want to see how your numerical estimation ability stacks up against your peers'?</h3>
                    <a href="http://panamath.org/" class="btn btn-lg btn-light">Take our test!</a>
                </div>
            </div>
        </div>
    </aside> -->

    <aside id="demo1" class="call-to-action bg-secondary">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h3>Abstract</h3>
                    <hr class="small">
                </div>
                <div class="col-lg-12 text-left"> 
                <p>A system for nonverbally representing the approximate number of items in visual and auditory arrays has been documented in multiple species, including humans. Although many aspects of this approximate number system are well characterized, fundamental questions remain unanswered: How does attention select which items in a scene to enumerate, and how many enumerations can be computed simultaneously? Here we show that when presented an array containing different numbers of spatially overlapping dots of many colors, human adults can select and enumerate items on the basis of shared color and can enumerate approximately three color subsets from a single glance. This three-set limit converges with previously observed three-item limits of parallel attention and visual short-term memory. This suggests that participants can select a subset of items from a complex array as a single individual set, which then serves as the input to the approximate number system.</p> 
                </div>
            </div>
        </div>
    </aside>

    <aside class="call-to-action bg-primary">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h3>General notes</h3>
                    <hr class="small">
                    <p> General notes on these demos: You will see a series of images of colored dots. Your task is to ignore the masks and report the number of dots in the set that is probed on each trial as best you can. The probe randomly appears either before or after the masks on each trial.</p>
                </div>
                <div class="col-lg-6 text-center">
                </div>
            </div>
            <div class="col-lg-6 text-center">
                <img id="demo5Img1" src="demoFiles/HSFGeneralTrialStructure.jpg" controls></img>
            </div>
            <div class="col-lg-6 text-center">
                <video id="demo5VideoExe" src="movies/6Trials.mov" controls width="320" height="240" scale="tofit"></video>
            </div>
            </div>
            </div>
        </div>
    </aside>

    <aside class="call-to-action bg-secondary">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h3>Superset Trials</h3>
                    <hr class="small">
                <p> When adults were asked to enumerate the superset of all dots they succeeded on both probe-before and probe-after trials suggesting that they always encoded the superset of all dots irrespective of the number of colors. Right click to play the below videos. </p>
                </div>
            <div class="col-lg-6 text-center">
                <video id="demo5video1" src="movies/3-ColorAllPBefore.mov" controls width="320" height="240" scale="tofit"></video>
                </div>
                <div class="col-lg-6 text-center">
                <video id="demo5video2" src="movies/5-ColorAllPAfter.mov" controls width="320" height="240" scale="tofit"></video>
                </div>
            </div>
           <div class="row">
                <div class="col-lg-12 text-center">
                    <h3><br> Color Subset Trials</h3>
                    <hr class="small">
                <p> On 1-Color trials, adults succeeded on both probe-before and probe-after trials demonstrating that this task contacts the Approximate Number System. On 2-Color trials, most adults succeeded on both probe-before and probe after trials. With arrays containing 3 or more colors, adults succeeded on probe-before trials but showed increased error on probe after trials (adults underestimated the number of dots in the probed set or failed to encode it). This limit of three sets suggests that participants could store each set as an individual in a single slot in visual short-term memory with numerosity stored as a feature of each set. </p>
                </div>
                <div class="col-lg-6 text-center">
                <video id="demo4video3" src="movies/1-ColorPBefore.mov" controls width="320" height="240" scale="tofit"></video> 
                </div>
                <div class="col-lg-6 text-center">
                <video id="demo4video4" src="movies/1-ColorPAfter.mov" controls width="320" height="240" scale="tofit"></video>
                </div>
                <div class="col-lg-6 text-center">
                <video id="demo4video5" src="movies/2-ColorPBefore.mov" controls width="320" height="240" scale="tofit"></video> 
                </div>
                <div class="col-lg-6 text-center">
                <video id="demo4video6" src="movies/2-ColorPAfter.mov" controls width="320" height="240" scale="tofit"></video>
                </div>
                <div class="col-lg-6 text-center">
                <video id="demo4video7" src="movies/4-ColorPBefore.mov" controls width="320" height="240" scale="tofit"></video> 
                </div>
                <div class="col-lg-6 text-center">
                <video id="demo4video8" src="movies/6-ColorPAfter.mov" controls width="320" height="240" scale="tofit"></video>
                </div>
        </div>
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h3><br>Experiments 2 &amp; 3</h3>
                    <hr class="small">
                <p> These experiments were similar to Exp 1, but the stimuli were controlled for total area (Exp 2) and circumference (Exp 3). As in Exp 1, adult, on average, encoded 3 sets in parallel. This suggests that responses were based on numerosity and not on some other visual feature correlated with numerosity.</p>
                </div>
            <div class="col-lg-6 text-center">
                <video id="demo5video9" src="movies/4-ColorPBefArea.mov" controls width="320" height="240" scale="tofit"></video> </div> 
                <div class="col-lg-6 text-center">
                <video id="demo5video10" src="movies/5-ColorPAftArea.mov" controls width="320" height="240" scale="tofit"></video> </div>
                <div class="col-lg-6 text-center">
                <video id="demo5video11" src="movies/5-ColorAllPBefCircum.mov" controls width="320" height="240" scale="tofit"></video> </div>
                <div class="col-lg-6 text-center">
                <video id="demo5video12" src="movies/6-ColorPAfCircum.mov" controls width="320" height="240" scale="tofit"></video> </div>
            </div>
        </div>
    </aside>



        <!-- Call to Action -->
    <aside id="demos" class="call-to-action bg-secondary">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h3>Demos</h3>
                    <hr class="small"> 
                </div>
                <div class="col-lg-12 text-left">    

                   <p>Halberda, J., Ly, R., Wilmer, J., Naiman, D., &amp; Germine, L. (2012). Number Sense across the lifespan as revealed by a massive internet-based sample. PNAS <a target="_blank" href="pnas2012/index.html">[Demo]</a> </p>

                    <p>Spiegel, C., &amp; Halberda, J. Rapid fast-mapping abilities in 2-year-olds. Journal of Experimental Child Psychology (2010). <a target="" href="SpiegelHalberda2011Demo">[Demo]</a></p>

                    <p>Halberda, J., Mazzocco, M. &amp; Feigenson, L. (2008). Individual differences in non-verbal number acuity correlate with maths achievement. Nature, 455, 665-668. <a target="" href="HalberdaMazzoccoFeigenson2008Demo.html">[Demo]</a></p>

                    <p>Halberda, J. &amp; Feigenson, L. (2008). Developmental Change in the Acuity of the Number Sense: The Approximate Number System in 3-, 4-, 5-, and 6-Year-Olds and Adults. Developmental Psychology, 44 (5). <a target="" href="HalberdaFeigenson2008Demo.html">[Demo]</a></p>

                    <p>Halberda, J., Taing, L. &amp; Lidz, J. (2008). The development of “most” comprehension and its potential dependence on counting-ability in preschoolers. Language Learning and Development. <a target="" href="HalberdaTaingLidz2008Demo.html">[Demo]</a> </p>

                    <p>Halberda, J., Sires, S.F., &amp; Feigenson, L. (2006). Multiple spatially-overlapping sets can be enumerated in parallel. Psychological Science. 17 (7), 572-576. <a target="" href="#top">[Demo]</a> </p>

                    <p> Halberda, J. (2006). Is this a dax which I see before me? Use of the logical argument disjunctive syllogism supports word-learning in children and adults. Cognitive Psychology, 53(4), 310-344. <a target="" href="Halberda2006Demo.html">[Demo]</a> </p>

                    <p>Nichols, S. &amp; Halberda, J. (in preparation). Modal reasoning in preschoolers. <a target="" href="NicholsHalberdaDemo.html">[Demo]</a></p>
                </div>
            </div>
        </div>
    </aside>

        <!-- Call to Action -->
    <aside id="joinus" class="call-to-action bg-primary">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h3>Interested in becoming a part of VCL?</h3>
                    <p> Prospective research assistants should email Tyler Knowlton (tknowlt2@jhu.edu); prospective graduate students should email Dr. Halberda directly (halberda@jhu.edu). </p> 
                </div>
            </div>
        </div>
    </aside>

   

   
    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-10 col-lg-offset-1 text-center">
                    <!-- <h4><strong>The Vision &amp; Cognition Lab</strong>
                    </h4> -->
                    <a target="" href="index.html"><img id="VCLdots" src="img/VCLlogo.jpg"> </a></img>
                    <a target="" href="http://www.psy.jhu.edu/~labforchilddevelopment/"><img src="img/development.jpg"> </a></img>
                    <a target="" href="http://pbs.jhu.edu/research/egeth/index.html"><img src="img/APLlogo.jpg"> </a></img>
                    <a target="" href="http://www.jhuvisualthinkinglab.com/"><img src="img/VTL.png"> </a></img>
                    <a target="" href="http://pbs.jhu.edu/research/fischer/"><img src="img/DynamicPerceptionLab.png"> </a></img>
                    <a target="" href="http://www.jhuvisionsciencesgroup.org/"><img src="img/VSG.jpeg"> </a></img>
                    <a target="_blank" href="https://www.jhu.edu/"><img src="img/JHU.png"></img></a> 
                </div>
            </div>
        </div>
    </footer>


   <aside id="end" class="call-to-action bg-primary">
        <div class="container">
            <div class="col-lg-10 col-lg-offset-1 text-center">
                    <p>The Vision &amp; Cognition Lab <br> Department of Psychological and Brain Sciences<br>
                    Johns Hopkins University, Ames Hall<br>
                    3400 North Charles St.<br>Baltimore, MD 21218</p>
                    <ul class="list-unstyled">
                        <li><i class="fa fa-phone fa-fw"></i> (410) 516-6175</li>
                        <li><i class="fa fa-envelope-o fa-fw"></i>  <a href="mailto:visionlabjhu@gmail.com"> <FONT COLOR=F0FFFF>visionlabjhu@gmail.com</FONT></a>
                        </li>
                    </ul>
                    <p class="text-muted"><br> <FONT COLOR=DCDCDC>&copy; JHU Vision &amp; Cognition Lab 2015</FONT></p>
                </div>
        </div>
    </aside>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script>
    // Closes the sidebar menu
    $("#menu-close").click(function(e) {
        e.preventDefault();
        $("#sidebar-wrapper").toggleClass("active");
    });

    // Opens the sidebar menu
    $("#menu-toggle").click(function(e) {
        e.preventDefault();
        $("#sidebar-wrapper").toggleClass("active");
    });

    // Scrolls to the selected menu item on the page
    $(function() {
        $('a[href*=#]:not([href=#])').click(function() {
            if (location.pathname.replace(/^\//, '') == this.pathname.replace(/^\//, '') || location.hostname == this.hostname) {

                var target = $(this.hash);
                target = target.length ? target : $('[name=' + this.hash.slice(1) + ']');
                if (target.length) {
                    $('html,body').animate({
                        scrollTop: target.offset().top
                    }, 1000);
                    return false;
                }
            }
        });
    });
    </script>

</body>

</html>
